OPENAI_API_KEY = "OPENAI_API_KEY_HERE"
anthropic_api_key = "ANTHROPIC_API_KEY_HERE"
qwen="QWEN_API_KEY_HERE"

# ============================================
# Ollama Local Model Configuration (Optional)
# ============================================
# Ollama enables running LLMs locally without cloud API dependencies.
# Install Ollama: https://ollama.com/download
#
# Base URL (default: http://localhost:11434/v1)
# OLLAMA_BASE_URL=http://localhost:11434/v1
#
# API Key (default: "ollama" - only needed for custom deployments)
# OLLAMA_API_KEY=ollama
#
# Model selection per size tier:
# Small models (7B-8B) - Fast, good for simple tasks
# OLLAMA_MODEL_SMALL=llama3.1:8b
# OLLAMA_MODEL_SMALL=mistral:7b
# OLLAMA_MODEL_SMALL=qwen2.5:7b
#
# Medium models (13B-20B) - Balanced performance
# OLLAMA_MODEL_MEDIUM=llama3.1:latest
# OLLAMA_MODEL_MEDIUM=mixtral:8x7b
# OLLAMA_MODEL_MEDIUM=qwen2.5:14b
#
# Large models (70B+) - Best reasoning, requires significant VRAM
# OLLAMA_MODEL_LARGE=llama3.1:70b
# OLLAMA_MODEL_LARGE=qwen2.5:72b